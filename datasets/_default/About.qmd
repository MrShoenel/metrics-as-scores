---
subtitle: "A Dataset For _Metrics As Scores_"
jupyter: python3
---

\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand\tightto{\!\to\!}
\newcommand\tightmapsto{\!\mapsto\!}
\newcommand{\tight}[1]{\,{#1}\,}
\newcommand{\utight}[1]{{#1}\,}

```{python}
#| echo: false

# You may edit this; it's the significance level used in this report.
use_alpha = 0.05

import pandas as pd
from os import getcwd
from pathlib import Path
from json import load
from IPython.display import display, Markdown
from tabulate import tabulate
from metrics_as_scores.distribution.distribution import Dataset, LocalDataset

def format_comma_and(lst: str) -> str:
    if len(lst) == 1:
        return lst[0]
    l = lst[0:(len(lst) - 1)]
    r = lst[len(lst) - 1]
    if len(l) == 1:
        return f'{l[0]} and {r}'
    return f'{", ".join(l)}, and {r}'

dm = lambda s: display(Markdown(s))

# Let's load the entire dataset, except for the original df!
dataset_dir = Path(getcwd())

df_anova = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/anova.csv')))
df_ks2samp = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/ks2samp.csv')))
df_tukey = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/tukeyhsd.csv')))
manifest: LocalDataset = None
with open(file=str(dataset_dir.joinpath('./manifest.json')), mode='r', encoding='utf-8') as fp:
	manifest = load(fp=fp)

dataset = Dataset(ds=manifest, df=pd.DataFrame())
```


```{python}
#| echo: false

dm(f"""
# Description
{manifest['desc']}
""")
```

This dataset has the following **Quantity Types**:

```{python}
#| echo: false

temp = [f'- *{qtype}*: {dataset.qytpe_desc(qtype=qtype)}' for qtype in dataset.quantity_types]

dm('\n'.join(temp))
```

```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))

dm(f'''
It has a total of {len(contexts)} **Contexts**: {format_comma_and(list([f'*{c}*' for c in contexts]))}.''')
```



# Analysis

In this section, results for the analysis of variance (ANOVA) and Tukey's Honest Significance Test (TukeyHSD) are shown.
These tests will give a first indication as to how different the quantity types are across contexts.
These two tests were used in the original paper [@honel2022qrs] that *Metrics As Scores* was initially conceived for.


These tests are conducted to help answering related questions, such as:

- Are there significant statistical differences for each type of quantity across all contexts?
- Is each context in its entirety (i.e., considering all types of quantities) distinguishable from the other contexts?
- What are good/bad or common/extreme scores for each context of the given dataset?


## ANOVA

This test analyzes the differences among means [@Chambers2017-fu].
For each type of quantity, this test analyzes if means of its samples are significantly different across contexts.
The null hypothesis of this test is that there are *no* significant differences.
This test yields a p-value and an F-statistic.
The latter is the mean square of each independent variable divided by the mean square of the residuals.
Large F-statistics indicate that the variation among contexts is likely.
The p-value then indicates how likely it is for the F-statistic to have occurred, given the null hypothesis is true.


## KS2

The Two-sample Kolmogorov--Smirnov Test (KS2) is a non-parametric and tests whether two samples stem from the same probability distribution [@stephens1974kstest].
KS2 does not check for a certain type of probability distribution since it uses the samples' empirical CDFs.
Its test statistic is the maximum vertical distance between the two CDFs.
For two samples $\mathbf{x},\mathbf{y}$, the statistic is calculated as $D_{\mathbf{x},\mathbf{y}}\tight{=}\sup_t\,\abs{F_{\mathcal{X}}(t)\tight{-}F_{\mathcal{Y}}(t)}$.
The null hypothesis is that the samples' CDFs are identical, that is, $F_{\mathcal{X}}\tight{=}F_{\mathcal{Y}}$.
This test is used to compare one type of quantity between two contexts.


## TukeyHSD

This test is used to gain insights into the results of an ANOVA test.
While the former only allows obtaining the amount of corroboration for the null hypothesis, TukeyHSD performs all pairwise comparisons [@tukey1949hsd].
For example, by choosing a certain type of quantity and context, we obtain a list of other contexts that are significantly statistically different.
The null hypothesis of this test is the same as for the ANOVA test.


# Results

Here we present some insights from conducting the ANOVA- KS2-, and TukeyHSD tests.


## ANOVA

@tbl-anova show the results of the ANOVA analysis.
For each type of quantity, it indicates whether the quantity types' means vary significantly across contexts.
For this test, we also add a virtual contexts, in which we simply merge the values of all contexts and effectively disregard the context.
Therefore, the ANOVA test also indicates whether quantity types' values are different in a specific context when compared to all recorded values.


```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))
row = df_anova.iloc[0,:].to_dict()


null_holds = row["pval"] > use_alpha
interpret: str=None
if null_holds:
	interpret = f'For a significance level of $\\alpha\\tight{{=}}{use_alpha}$, the null hypothesis cannot be rejected, meaning that the means of samples of the quantity type {row["qtype"]} are not statistically significantly different across contexts.'
else:
	interpret = f'We cannot accept the null hypothesis for a significance level of $\\alpha\\tight{{=}}{use_alpha}$, meaning that the the means of samples of the quantity type {row["qtype"]} have statistically significantly different means across contexts.'


dm(f'''
@tbl-anova shows, for each quantity type, if there was corroboration for the null hypothesis, which here is whether the samples of the same quantity type have the same mean across contexts.
So, for example, samples of the first quantity type **{row["qtype"]}** were compared across
contexts {format_comma_and(list([f"*{c}*" for c in row["across_contexts"].split(";")]))}.
{interpret}
'''.strip())
```



```{python}
#| label: tbl-anova
#| tbl-cap: Results of the ANOVA analysis.
#| echo: false

temp = df_anova[['qtype', 'pval', 'stat']]
Markdown(tabulate(tabular_data=temp, headers=['Quantity Type', 'p-Value', 'F-Statistic'], showindex=False))
```



## KS2

```{python}
#| echo: false

n = len(contexts) + 1
r = int(n * (n - 1) / 2)

dm(f'''
The two-sample Kolmogorov--Smirnov test checks if two samples stem from an identical distribution (which is the null hypothesis).
The KS2 test requires pairwise comparisons.
For each type of quantity, we compare it within all the contexts.
Therefore, all pair-wise combinations are computed.
This dataset has ${n-1}$ contexts plus one virtual (in which we effectively disregard the context, therefore, $n\\tight{{=}}{n}$).
The number of pair-wise comparisons per type of quantity, therefore, is $n\\tight{{\\times}}(n\\tight{{-}}1)\\tight{{\\div}}2\\tight{{=}}{r}$.
In other words, each type of quantity can significantly stick out anywhere between zero and ${r}$ times.
@fig-ks2 shows the results of the KS2 test using the significance threshold $\\alpha\\tight{{=}}{use_alpha}$.
'''.strip())
```


```{python}
#| label: fig-ks2
#| fig-cap: The frequency with which quantity types were considered to come from the same distribution. Numbers close to the max indicate that the type of quantity is not significantly different across contexts.
#| echo: false

import matplotlib
import matplotlib.pyplot as plt
from bokeh.palettes import Category20_20
from math import ceil

temp = df_ks2samp[df_ks2samp.pval >= use_alpha]
cnts = list([
	temp[(temp.qtype == qtype)].shape[0]
	for qtype in dataset.quantity_types
])

fig, ax = plt.subplots()
fig.set_figwidth(8)
fig.set_figheight(2.5)
bars = ax.bar(dataset.quantity_types, cnts, width=.5, color=Category20_20)

ax.set_ylim(bottom=0, top=r + ceil(r*.1))
ax.bar_label(bars)
ax.set_axisbelow(True)
ax.grid(color='#dddddd', linestyle='dashed')

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Computer Modern Roman"
})
plt.title("Frequency with which metrics are similar across contexts.")
plt.show()
```

<!--
The results of the KS2 tests are illustrated in Figure~\ref{fig:rq1_ks2}.
For each metric, it shows the number of times the metric was \emph{not} distributed statistically significantly different in other domains.
Seven metrics (\cpvar{DIT}, \cpvar{MLOC}, \cpvar{NBD}, \cpvar{NOF}, \cpvar{NOM}, \cpvar{VG}, and \cpvar{WMC}) have a count of zero.
Those metrics are therefore \emph{always} different, across all $66$ pairwise comparisons.
The average of how often a metric was similar is $\utight{\approx}9.43$ times.
The metrics \cpvar{PAR}, \cpvar{SIX}, \cpvar{NSF}, \cpvar{NORM}, \cpvar{LCOM}, \cpvar{RMD}, \cpvar{CA}, and \cpvar{RMI} are rarely similar with a frequency in the range $\interval{1}{11}$.
The metrics \cpvar{NSC}, \cpvar{NSM}, \cpvar{RMA}, \cpvar{NOC}, \cpvar{CE}, and \cpvar{NOI} are moderately often similar with a frequency between $\interval{14}{21}$.
The metrics \cpvar{NOP} ($29$) and \cpvar{TLOC} ($34$) on the other hand are frequently similar.
-->



<!--
	The following is the cross-platform solution for correctly showing references.
-->
# References {-}

<div id="refs"></div>

<!--
# Bla `ds.ds["name"]`{.python}

For a demonstration of a line plot on a polar axis, see @fig-polar.

```{python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

-->