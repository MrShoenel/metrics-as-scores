---
subtitle: "A Dataset For _Metrics As Scores_"
jupyter: python3
---

\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand\tightto{\!\to\!}
\newcommand\tightmapsto{\!\mapsto\!}
\newcommand{\tight}[1]{\,{#1}\,}
\newcommand{\utight}[1]{{#1}\,}

```{python}
#| echo: false

# You may edit this; it's the significance level used in this report.
use_alpha = 0.05

import pandas as pd
from os import getcwd
from pathlib import Path
from json import load
from IPython.display import display, Markdown
from tabulate import tabulate
from metrics_as_scores.distribution.distribution import Dataset, LocalDataset

def format_comma_and(lst: str) -> str:
    if len(lst) == 1:
        return lst[0]
    l = lst[0:(len(lst) - 1)]
    r = lst[len(lst) - 1]
    if len(l) == 1:
        return f'{l[0]} and {r}'
    return f'{", ".join(l)}, and {r}'

dm = lambda s: display(Markdown(s))

# Let's load the entire dataset, except for the original df!
dataset_dir = Path(getcwd())

df_anova = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/anova.csv')))
df_ks2samp = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/ks2samp.csv')))
df_tukey = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/tukeyhsd.csv')))
manifest: LocalDataset = None
with open(file=str(dataset_dir.joinpath('./manifest.json')), mode='r', encoding='utf-8') as fp:
	manifest = load(fp=fp)

dataset = Dataset(ds=manifest, df=pd.DataFrame())
```


```{python}
#| echo: false

dm(f"""
# Description
{manifest['desc']}
""")
```

This dataset has the following **Quantity Types**:

```{python}
#| echo: false

temp = [f'- *{qtype}*: {dataset.qytpe_desc(qtype=qtype)}' for qtype in dataset.quantity_types]

dm('\n'.join(temp))
```

```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))

dm(f'''
It has a total of {len(contexts)} **Contexts**: {format_comma_and(list([f'*{c}*' for c in contexts]))}.''')
```



# Analysis

In this section, results for the analysis of variance (ANOVA) and Tukey's Honest Significance Test (TukeyHSD) are shown.
These tests will give a first indication as to how different the quantity types are across contexts.
These two tests were used in the original paper [@honel2022qrs] that *Metrics As Scores* was initially conceived for.


These tests are conducted to help answering related questions, such as:

- Are there significant statistical differences for each type of quantity across all contexts?
- Is each context in its entirety (i.e., considering all types of quantities) distinguishable from the other contexts?
- What are good/bad or common/extreme scores for each context of the given dataset?


## ANOVA

This test analyzes the differences among means [@Chambers2017-fu].
For each type of quantity, this test analyzes if means of its samples are significantly different across contexts.
The null hypothesis of this test is that there are *no* significant differences.
This test yields a p-value and an F-statistic.
The latter is the mean square of each independent variable divided by the mean square of the residuals.
Large F-statistics indicate that the variation among contexts is likely.
The p-value then indicates how likely it is for the F-statistic to have occurred, given the null hypothesis is true.


## KS2

The Two-sample Kolmogorov--Smirnov Test (KS2) is a non-parametric and tests whether two samples stem from the same probability distribution [@stephens1974kstest].
KS2 does not check for a certain type of probability distribution since it uses the samples' empirical CDFs.
Its test statistic is the maximum vertical distance between the two CDFs.
For two samples $\mathbf{x},\mathbf{y}$, the statistic is calculated as $D_{\mathbf{x},\mathbf{y}}\tight{=}\sup_t\,\abs{F_{\mathcal{X}}(t)\tight{-}F_{\mathcal{Y}}(t)}$.
The null hypothesis is that the samples' CDFs are identical, that is, $F_{\mathcal{X}}\tight{=}F_{\mathcal{Y}}$.
This test is used to compare one type of quantity between two contexts.


## TukeyHSD

This test is used to gain insights into the results of an ANOVA test.
While the former only allows obtaining the amount of corroboration for the null hypothesis, TukeyHSD performs all pairwise comparisons [@tukey1949hsd].
For example, by choosing a certain type of quantity and context, we obtain a list of other contexts that are significantly statistically different.
The null hypothesis of this test is the same as for the ANOVA test.


# Results

Here we present some insights from conducting the ANOVA- KS2-, and TukeyHSD tests.


## ANOVA

@tbl-anova show the results of the ANOVA analysis.
For each type of quantity, it indicates whether the quantity types' means vary significantly across contexts.
For this test, we also add a virtual contexts, in which we simply merge the values of all contexts and effectively disregard the context.
Therefore, the ANOVA test also indicates whether quantity types' values are different in a specific context when compared to all recorded values.


```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))
row = df_anova.iloc[0,:].to_dict()


null_holds = row["pval"] > use_alpha
interpret: str=None
if null_holds:
	interpret = f'For a significance level of $\\alpha\\tight{{=}}{use_alpha}$, the null hypothesis cannot be rejected, meaning that the means of samples of the quantity type {row["qtype"]} are not statistically significantly different across contexts.'
else:
	interpret = f'We cannot accept the null hypothesis for a significance level of $\\alpha\\tight{{=}}{use_alpha}$, meaning that the the means of samples of the quantity type {row["qtype"]} have statistically significantly different means across contexts.'


dm(f'''
@tbl-anova shows, for each quantity type, if there was corroboration for the null hypothesis, which here is whether the samples of the same quantity type have the same mean across contexts.
So, for example, samples of the first quantity type **{row["qtype"]}** were compared across
contexts {format_comma_and(list([f"*{c}*" for c in row["across_contexts"].split(";")]))}.
{interpret}
'''.strip())
```



```{python}
#| label: tbl-anova
#| tbl-cap: Results of the ANOVA analysis.
#| echo: false

temp = df_anova[['qtype', 'pval', 'stat']]
Markdown(tabulate(tabular_data=temp, headers=['Quantity Type', 'p-Value', 'F-Statistic'], showindex=False))
```



## KS2

```{python}
#| echo: false

n = len(contexts) + 1
r = int(n * (n - 1) / 2)

dm(f'''
The two-sample Kolmogorov--Smirnov test checks if two samples stem from an identical distribution (which is the null hypothesis).
The KS2 test requires pairwise comparisons.
For each type of quantity, we compare it within all the contexts.
Therefore, all pair-wise combinations are computed.
This dataset has ${n-1}$ contexts plus one virtual (in which we effectively disregard the context, therefore, $n\\tight{{=}}{n}$).
The number of pair-wise comparisons per type of quantity, therefore, is $n\\tight{{\\times}}(n\\tight{{-}}1)\\tight{{\\div}}2\\tight{{=}}{r}$.
In other words, each type of quantity can significantly stick out anywhere between zero and ${r}$ times.
@fig-ks2 shows the results of the KS2 test using the significance threshold $\\alpha\\tight{{=}}{use_alpha}$.
'''.strip())
```


```{python}
#| label: fig-ks2
#| fig-cap: The frequency with which quantity types were considered to come from the same distribution. Numbers close to the max indicate that the type of quantity is not significantly different across contexts.
#| echo: false

import matplotlib
import matplotlib.pyplot as plt
from bokeh.palettes import Category20_20
from math import ceil

temp = df_ks2samp[df_ks2samp.pval >= use_alpha]
cnts = list([
	temp[(temp.qtype == qtype)].shape[0]
	for qtype in dataset.quantity_types
])

fig, ax = plt.subplots()
fig.set_figwidth(8)
fig.set_figheight(2.5)
bars = ax.bar(dataset.quantity_types, cnts, width=.5, color=Category20_20)

ax.set_ylim(bottom=0, top=r + ceil(r*.1))
ax.bar_label(bars)
ax.set_axisbelow(True)
ax.grid(color='#dddddd', linestyle='dashed')

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Computer Modern Roman"
})
plt.title("Frequency with which metrics are similar across contexts.")
plt.show()
```



## TukeyHSD


```{python}
#| echo: false

n = len(contexts) + 1
m = len(dataset.quantity_types)
r = (n-1) * m

dm(f'''
Per context, this test allows us to report how many types of quantities are different, compared to all other contexts' types of quantities.
Consider the set of contexts $\\mathit{{C}}$ and the set of quantity types $\\mathit{{Q}}$.
Given a context $c_i$ and a quantity type $q_j$, this test reports all other contexts $\\mathit{{C}}^{{\\setminus c_i}}$ that have a statistically significantly different distribution for $q_j$.
However, for $c_i$, we aggregate these counts across all quantity types.
We have a total of $n\\tight{{=}}{n-1}+1$ contexts (including the virtual context).
Therefore, the amount of quantity types ($m\\tight{{=}}{len(dataset.quantity_types)}$) that are different in different contexts can maximally be $(n\\tight{{-}}1)\\tight{{\\times}}m\\tight{{=}}{r}$.
'''.strip())
```

```{python}
#| label: fig-tukey
#| fig-cap: Number of quantity types that are different per context, by comparing each context to all other contexts.
#| echo: false
#| warning: false

import numpy as np

temp = list()
for idx in range(len(df_tukey.index)):
	row = df_tukey.iloc[idx,:].to_dict()
	if row['reject']:
		# Reject null hypothesis -> qtype's contexts are different
		temp.append(row['group1'])
		temp.append(row['group2'])

values, counts = np.unique(np.array(temp), axis=0, return_counts=True)
values, counts = list(values), list(counts)

cnt_dict = {}
for idx, val in enumerate(values):
	cnt_dict[val] = counts[idx]

cnts = list()
for ctx in dataset.contexts(include_all_contexts=True):
	if ctx in cnt_dict:
		cnts.append(cnt_dict[ctx])
	else:
		cnts.append(0)

fig, ax = plt.subplots()
fig.set_figwidth(8)
fig.set_figheight(2.5)
bars = ax.bar(list(dataset.contexts(include_all_contexts=False)) + ['\\textbf{ALL}'], cnts, width=.5, color=Category20_20)

r = max(list(cnt_dict.values()))
ax.set_ylim(bottom=0, top=r + ceil(r*.1))
ax.bar_label(bars)
ax.set_axisbelow(True)
ax.grid(color='#dddddd', linestyle='dashed')
angle = 0 if len(contexts) < 10 else (45 if len(contexts) < 20 else 90)
ax.set_xticklabels(ax.get_xticklabels(), rotation=angle)

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Computer Modern Roman"
})
plt.title("Number of quantity types different per context.")
plt.show()
```


## Common/extreme Values

In this section, we demonstrate some common or extreme values by showing what (maximum) distance is required to achieve a score as shown in header of @tbl-common-extreme.
The example shown here is that of the first type of quantity, which has been transformed using each context's expection, $\mathbb{E}[X]$.
Since it is practically unlikely to have uniformly distributed quantities, a linear increase in score is usually associated with a non-linear decrease in distance.
Note that the values in @tbl-common-extreme are approximate.


```{python}
#| echo: false

# Let's read the file 'densities_KDE_approx_EXPECTATION' so that we can generate some table.
# This is one example of how to do this, but feel free to generate any kind of table, using
# any type of density, or even show multiple tables.

from metrics_as_scores.distribution.distribution import KDE_approx
from pickle import load

dens: dict[str, KDE_approx] = None
with open(file=str(dataset_dir.joinpath('./densities/densities_KDE_approx_EXPECTATION.pickle')), mode='rb') as fp:
	dens = load(file=fp)
```

```{python}
#| label: tbl-common-extreme
#| tbl-cap: The required maximum distance from the context's expectation $\mathbb{E}[X]$ to achieve a score less than or equal to $x$. The __ALL__-context is shown as well.
#| echo: false

import numpy as np
from typing import Callable
from scipy.optimize import direct, minimize
from scipy.integrate import quad
from numpy import vectorize

# Quantiles [8+1]: 0, 0.1, 0.25, 0.5, 0.75, 0.85, 0.95, 0.99, E[X]
use_quantiles = np.array([0.001, 0.1, 0.2, 0.33, 0.5, 0.66, 0.75, 0.85, 0.9, 0.999])


def find_score_pdf(score: float, dist: KDE_approx):
	pdf_scale, _ = quad(func=dist.pdf, a=0, b=dist._range_data[1] * 10)
	pdf = lambda x: dist.pdf(x) / pdf_scale
	ccdf = lambda x: 1.0 - quad(func=pdf, a=0, b=x)[0]
	#res = direct(func=lambda x: (score - ccdf(x))**2, bounds=((0,dist._range_data[1] * 10),))
	res = minimize(fun=lambda x: (score - ccdf(x))**2, x0=0.001, bounds=((0,dist._range_data[1] * 10),))
	return res.x[0]

find_score_pdf = vectorize(find_score_pdf)

templ = []
use_qty = dataset.quantity_types[0]
for ctx in dataset.contexts(include_all_contexts=True):
	dist = dens[f'{ctx}_{use_qty}']
	tf = dist.transform_value
	templ.append([ctx] + list(['{:.2e}'.format(r) for r in list(find_score_pdf(use_quantiles, dist)) + [tf]]))

Markdown(tabulate(tabular_data=pd.DataFrame(templ), headers=[dataset.ds['colname_context']] + list(str(s) for s in list(use_quantiles) + ['E[X]']), showindex=False))
```


<!--
	The following is the cross-platform solution for correctly showing references.
-->
# References {-}

<div id="refs"></div>

<!--
# Bla `ds.ds["name"]`{.python}

For a demonstration of a line plot on a polar axis, see @fig-polar.

```{python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

-->