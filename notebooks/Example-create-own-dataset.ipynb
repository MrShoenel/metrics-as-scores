{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: Create Own Dataset\n",
    "=================================\n",
    "\n",
    "## The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will demonstrate how to use an own dataset with Metrics As Scores.\n",
    "Note that it is not required to interact with code, so we show two ways to achieve our goal here:\n",
    "\n",
    "1. Use the Text-based Command Line User Interface (TUI)\n",
    "2. Implement the same scenario in code\n",
    "\n",
    "The latter might be useful for programmatic usage, when you need to create datasets in a batch fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Text-based Command Line User Interface (TUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Creation of Own Dataset in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will go through the following steps:\n",
    "\n",
    "1. Load the Iris data frame and transform it into the required format.\n",
    "   1. Transforming the data frame\n",
    "   2. Creating a manifest\n",
    "2. Conduct analyses that are required for generating a scientific report.\n",
    "3. Fitting of parametric random variables to the data.\n",
    "4. Generating the densities required for the web application of Metrics As Scores.\n",
    "5. Finishing Up\n",
    "   \n",
    "\n",
    "Please note: The documentation for the code can be found at <https://mrshoenel.github.io/metrics-as-scores/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset\n",
    "\n",
    "The well-known Iris dataset is included in `scikit-learn`, so we can load it directly.\n",
    "It has 4 real-valued features and one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
      "0                5.1               3.5  ...               0.2  setosa\n",
      "1                4.9               3.0  ...               0.2  setosa\n",
      "2                4.7               3.2  ...               0.2  setosa\n",
      "3                4.6               3.1  ...               0.2  setosa\n",
      "4                5.0               3.6  ...               0.2  setosa\n",
      "\n",
      "[5 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "\n",
    "# Let's rename the numeric column and use the actual labels:\n",
    "for idx in range(len(iris.target_names)):\n",
    "    df['target'].replace(to_replace=idx, value=iris.target_names[idx], inplace=True)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data frame\n",
    "\n",
    "Metrics As Scores requires a stacked format of the data frame. We take each feature's data, the group, and the name of the feature to produce a new 3-column data frame. After we have done this for all features, we vertically stack these frames. This is implemented as a helper function, so we can just go ahead and make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature   Group  Value\n",
      "0  sepal length (cm)  setosa    5.1\n",
      "1  sepal length (cm)  setosa    4.9\n",
      "2  sepal length (cm)  setosa    4.7\n",
      "3  sepal length (cm)  setosa    4.6\n",
      "4  sepal length (cm)  setosa    5.0\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.tools.funcs import transform_to_MAS_dataset\n",
    "\n",
    "df_mas = transform_to_MAS_dataset(df=df, group_col='target', feature_cols=iris.feature_names)\n",
    "\n",
    "print(df_mas.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Manifest\n",
    "\n",
    "Every dataset must be accompanied by a manifest that provides some meta information about it. The manifest is also required for the next steps. When bundling and publishing a dataset, the manifest is a required file.\n",
    "\n",
    "Metrics As Scores used a typed dictionary that is then stored as `manifest.json`. Then, a new instance of `Dataset` is created. It is passed the transformed data frame and the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': ['First A. Author', 'Second Author'],\n",
       " 'id': 'iris',\n",
       " 'colname_context': 'Group',\n",
       " 'colname_data': 'Value',\n",
       " 'colname_type': 'Feature',\n",
       " 'qtypes': {'sepal length (cm)': 'Description for feature sepal length (cm)',\n",
       "  'sepal width (cm)': 'Description for feature sepal width (cm)',\n",
       "  'petal length (cm)': 'Description for feature petal length (cm)',\n",
       "  'petal width (cm)': 'Description for feature petal width (cm)'},\n",
       " 'contexts': ['setosa', 'versicolor', 'virginica']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_as_scores.distribution.distribution import Dataset, LocalDataset\n",
    "\n",
    "# Here, we manually create a manifest and fill out the minimum required properties.\n",
    "# A manifest for a publishable dataset needs to present proper values for all keys.\n",
    "manifest: LocalDataset = {}\n",
    "manifest['author'] = ['First A. Author', 'Second Author']\n",
    "manifest['id'] = 'iris'\n",
    "manifest['colname_context'] = 'Group'\n",
    "manifest['colname_data'] = 'Value'\n",
    "manifest['colname_type'] = 'Feature'\n",
    "# Note this is a dictionary, where the keys are the features' (column) names\n",
    "# and the values are a obligatory descriptions.\n",
    "manifest['qtypes'] = { fn: f'Description for feature {fn}' for fn in iris.feature_names }\n",
    "# This is a list of the available groups.\n",
    "manifest['contexts'] = iris.target_names.tolist()\n",
    "\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the actual dataset for Metrics As Scores. The `Dataset` allows us to conduct the required statistical tests effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "dataset_mas = Dataset(ds=manifest, df=df_mas)\n",
    "# Just a test:\n",
    "print(dataset_mas.quantity_types_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct analyses that are required for generating a scientific report\n",
    "\n",
    "Here, we will conduct the three statistical analyses that are required to generate a report.\n",
    "The report itself is a Quarto template that can (but does not necessarily have to) be changed.\n",
    "Having the manifest, dataset, and the following three reports is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1001.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtype</th>\n",
       "      <th>stat</th>\n",
       "      <th>pval</th>\n",
       "      <th>across_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>44.194514</td>\n",
       "      <td>1.250402e-23</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>24.727043</td>\n",
       "      <td>2.637466e-14</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>87.738074</td>\n",
       "      <td>1.219770e-40</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>85.564667</td>\n",
       "      <td>6.874646e-40</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qtype  ...                      across_contexts\n",
       "0  sepal length (cm)  ...  setosa;versicolor;virginica;__ALL__\n",
       "1   sepal width (cm)  ...  setosa;versicolor;virginica;__ALL__\n",
       "2  petal length (cm)  ...  setosa;versicolor;virginica;__ALL__\n",
       "3   petal width (cm)  ...  setosa;versicolor;virginica;__ALL__\n",
       "\n",
       "[4 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we want to do a full analysis and, therefore, pass in all available features\n",
    "# (called quantity types) and groups (called contexts). We also pass in a virtual \"ALL\"-\n",
    "# context, which creates an additional group that contains all data.\n",
    "test_anova = dataset_mas.analyze_ANOVA(\n",
    "    qtypes=dataset_mas.quantity_types, contexts=list(dataset_mas.contexts(include_all_contexts=True)))\n",
    "\n",
    "test_anova.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2005.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>meandiff</th>\n",
       "      <th>p-adj</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>reject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>setosa</td>\n",
       "      <td>-0.8373</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.1287</td>\n",
       "      <td>-0.5460</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.8441</td>\n",
       "      <td>-0.1987</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.7447</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4533</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>setosa</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>1.2868</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>setosa</td>\n",
       "      <td>virginica</td>\n",
       "      <td>1.5820</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2252</td>\n",
       "      <td>1.9388</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group1      group2  meandiff   p-adj   lower   upper  reject\n",
       "0  __ALL__      setosa   -0.8373  0.0000 -1.1287 -0.5460    True\n",
       "1  __ALL__  versicolor    0.0927  0.8441 -0.1987  0.3840   False\n",
       "2  __ALL__   virginica    0.7447  0.0000  0.4533  1.0360    True\n",
       "3   setosa  versicolor    0.9300  0.0000  0.5732  1.2868    True\n",
       "4   setosa   virginica    1.5820  0.0000  1.2252  1.9388    True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tukey's Honest Significance Test:\n",
    "test_tukey = dataset_mas.analyze_TukeyHSD(qtypes=dataset_mas.quantity_types)\n",
    "\n",
    "test_tukey.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1976.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtype</th>\n",
       "      <th>stat</th>\n",
       "      <th>pval</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.807571e-15</td>\n",
       "      <td>setosa</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.773164e-23</td>\n",
       "      <td>setosa</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.537645e-11</td>\n",
       "      <td>setosa</td>\n",
       "      <td>__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.807534e-06</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.411500e-02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>__ALL__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qtype  stat          pval      group1      group2\n",
       "0  sepal length (cm)  0.78  2.807571e-15      setosa  versicolor\n",
       "1  sepal length (cm)  0.92  7.773164e-23      setosa   virginica\n",
       "2  sepal length (cm)  0.56  2.537645e-11      setosa     __ALL__\n",
       "3  sepal length (cm)  0.50  4.807534e-06  versicolor   virginica\n",
       "4  sepal length (cm)  0.24  2.411500e-02  versicolor     __ALL__"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The two-sample Komolgorov--Smirnov Test:\n",
    "test_ks2 = dataset_mas.analyze_distr(qtypes=dataset_mas.quantity_types, use_ks_2samp=True)\n",
    "\n",
    "test_ks2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting of parametric random variables to the data.\n",
    "\n",
    "Fitting random variables is required so that we can inspect and use the parametric fits in the web application.\n",
    "A dataset that ought to be published needs to contain these.\n",
    "Metrics As Scores can fit more than ~$120$ random variables, of which ~$20$ are discrete.\n",
    "The Iris dataset only has real-valued (continuous) features, so we will limit ourselves to continuous random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from nptyping import Float, NDArray, Shape\n",
    "from metrics_as_scores.distribution.fitting import FitterPymoo\n",
    "from metrics_as_scores.distribution.distribution import DistTransform\n",
    "from metrics_as_scores.data.pregenerate_fit import get_data_tuple\n",
    "from metrics_as_scores.data.pregenerate_distns import generate_parametric_fits\n",
    "from scipy.stats._distn_infrastructure import rv_generic\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_data_tuples(dist_transform: DistTransform, continuous: bool) -> tuple[dict[str, float], dict[str, NDArray[Shape[\"*\"], Float]]]:\n",
    "        \"\"\"\n",
    "        This function helps us to prepare all combinations of data that are required\n",
    "        for fitting. It was mainly taken from cli/FitParametric::_get_data_tuples(..),\n",
    "        so please go there if you need to know more.\n",
    "        \"\"\"\n",
    "        res = Parallel(n_jobs=-1)(delayed(get_data_tuple)(ds=dataset_mas, qtype=qtype, dist_transform=dist_transform, continuous_transform=continuous) for qtype in tqdm(dataset_mas.quantity_types))\n",
    "        data_dict = dict([(item[0], item[1]) for sublist in res for item in sublist])\n",
    "        transform_values_dict = dict([(item[0], item[2]) for sublist in res for item in sublist])\n",
    "        return (transform_values_dict, data_dict)\n",
    "\n",
    "\n",
    "def fit_parametric(dist_transform: DistTransform, selected_rvs_c: list[type[rv_generic]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    This function was also taken from cli/FitParametric.\n",
    "    \"\"\"\n",
    "    print('Performing distribution transforms for continuous random variables ...')\n",
    "    transform_values_dict, data_dict = get_data_tuples(dist_transform=dist_transform, continuous=True)\n",
    "    # We don't have discrete features, so let's use empty dictionaries.\n",
    "    # Please refer to the original function in FitParametric for how to\n",
    "    # do the same for discrete features, but it's pretty straightforward.\n",
    "    transform_values_discrete_dict, data_discrete_dict = {}, {}\n",
    "\n",
    "    print(f'Starting fitting of distributions for transform {dist_transform.value}, in randomized order.')\n",
    "    return generate_parametric_fits(\n",
    "        ds=dataset_mas,\n",
    "        num_jobs=-1,\n",
    "        fitter_type=FitterPymoo,\n",
    "        dist_transform=dist_transform,\n",
    "        selected_rvs_c=selected_rvs_c,\n",
    "        selected_rvs_d=[], # No discrete fits this time ;)\n",
    "        data_dict=data_dict,\n",
    "        data_discrete_dict=data_discrete_dict,\n",
    "        transform_values_dict=transform_values_dict,\n",
    "        transform_values_discrete_dict=transform_values_discrete_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each `DistTransform`, we generate a set of parametric fits.\n",
    "Instead of attempting to fit all available continuous random variables (from `Continuous_RVs`), we will make a dedicated list of random variables that should be attempted fitting, just for the sake of computing this notebook faster.\n",
    "In reality you should always attempt to fit them all, and you even must do so if you intend to publish your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2002.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform NONE [<none>], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:04<00:00, 88.41it/s] \n",
      "100%|██████████| 4/4 [00:00<00:00, 4005.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform EXPECTATION [E[X] (expectation)], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:04<00:00, 92.19it/s] \n",
      "100%|██████████| 4/4 [00:00<00:00, 3972.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform MEDIAN [Median (50th percentile)], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:04<00:00, 87.92it/s] \n",
      "100%|██████████| 4/4 [00:00<00:00, 3902.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform MODE [Mode (most likely value)], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:04<00:00, 89.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3929.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform INFIMUM [Infimum (min. observed value)], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:03<00:00, 109.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4114.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform SUPREMUM [Supremum (max. observed value)], in randomized order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:03<00:00, 102.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.distribution.fitting import Continuous_RVs # There is also 'Discrete_RVs' if you need them.\n",
    "from scipy.stats._continuous_distns import alpha_gen, cauchy_gen, crystalball_gen, exponnorm_gen, fisk_gen, foldcauchy_gen, genlogistic_gen, gumbel_l_gen, gumbel_r_gen, johnsonsb_gen, johnsonsu_gen, kstwobign_gen, laplace_asymmetric_gen, moyal_gen, nakagami_gen, ncf_gen, nct_gen, norm_gen, pearson3_gen, rayleigh_gen, rdist_gen, reciprocal_gen, rice_gen, skew_norm_gen, truncweibull_min_gen\n",
    "\n",
    "# Note that this is a list of random variables that were previously observed to best fit\n",
    "# (transformations of) the Iris data, it is not just made up!\n",
    "selected_rvs_c = [alpha_gen, cauchy_gen, crystalball_gen, exponnorm_gen, fisk_gen, foldcauchy_gen, genlogistic_gen, gumbel_l_gen, gumbel_r_gen, johnsonsb_gen, johnsonsu_gen, kstwobign_gen, laplace_asymmetric_gen, moyal_gen, nakagami_gen, ncf_gen, nct_gen, norm_gen, pearson3_gen, rayleigh_gen, rdist_gen, reciprocal_gen, rice_gen, skew_norm_gen, truncweibull_min_gen]\n",
    "\n",
    "fits: dict[DistTransform, list[dict[str, Any]]] = {}\n",
    "\n",
    "for dist_transform in list(DistTransform):\n",
    "    # You should actually store the result in a file with the following name:\n",
    "    file_name = f'pregen_distns_{dist_transform.name}.pickle'\n",
    "    \n",
    "    result = fit_parametric(dist_transform=dist_transform, selected_rvs_c=selected_rvs_c)\n",
    "    # Here, we'll just store the result in the above fits-dictionary:\n",
    "    fits[dist_transform] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a fit.\n",
    "You see that some best-fitting random variable was found (selected by the most appropriate statistical test).\n",
    "A variety of statistical tests was performed and each `FitResult` retains all of their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'setosa',\n",
       " 'dist_transform': 'EXPECTATION',\n",
       " 'qtype': 'sepal width (cm)',\n",
       " 'rv': 'fisk_gen',\n",
       " 'type': 'continuous',\n",
       " 'grid_idx': 29,\n",
       " 'transform_value': 3.428000000000001,\n",
       " 'params': {'c': 0.8316874605020181,\n",
       "  'loc': 0.028000000000000906,\n",
       "  'scale': 0.0557721052353025},\n",
       " 'stat_tests': {'tests': {'cramervonmises_ordinary': {'pval': 3.560315283823723e-05,\n",
       "    'stat': 1.7544763557922332},\n",
       "   'cramervonmises_jittered': {'pval': 3.560315283823723e-05,\n",
       "    'stat': 1.7544763557922332},\n",
       "   'cramervonmises_2samp_ordinary': {'pval': 0.004611473102710151,\n",
       "    'stat': 0.882200000000001},\n",
       "   'cramervonmises_2samp_jittered': {'pval': 0.004611473102710151,\n",
       "    'stat': 0.882200000000001},\n",
       "   'ks_1samp_ordinary': {'pval': 6.665666358245721e-06,\n",
       "    'stat': 0.3475925934892153},\n",
       "   'ks_1samp_jittered': {'pval': 6.665666358245721e-06,\n",
       "    'stat': 0.3475925934892153},\n",
       "   'ks_2samp_ordinary': {'pval': 0.005841778142694731, 'stat': 0.34},\n",
       "   'ks_2samp_jittered': {'pval': 0.005841778142694731, 'stat': 0.34},\n",
       "   'epps_singleton_2samp_ordinary': {'pval': 0.002229612727441589,\n",
       "    'stat': 16.680523275772053},\n",
       "   'epps_singleton_2samp_jittered': {'pval': 0.002229612727441589,\n",
       "    'stat': 16.680523275772053}},\n",
       "  'discrete_data1': False,\n",
       "  'discrete_data2': False}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fits[DistTransform.EXPECTATION][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the densities required for the web application of Metrics As Scores\n",
    "\n",
    "In order to use web application with our dataset, we need to pre-generate densities for it, as generating these during runtime could take a lot of time (depending on the dataset size) and be detrimental to the user experience.\n",
    "Therefore, we pre-generate these files and trade storage space for computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metrics_as_scores.data.pregenerate import generate_parametric, generate_empirical, generate_empirical_discrete\n",
    "from metrics_as_scores.distribution.distribution import Parametric, Parametric_discrete, Empirical, Empirical_discrete, KDE_approx\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "# The following functions were taken from cli/GenerateDensitiesWorkflow.\n",
    "# They are helpers for creating every required combination of density\n",
    "# type and DistTransform. Also, they do this in parallel.\n",
    "\n",
    "def generate_parametric() -> None:\n",
    "    grid = dict(\n",
    "        clazz = [Parametric, Parametric_discrete],\n",
    "        transform = list(DistTransform))\n",
    "    expanded_grid = pd.DataFrame(ParameterGrid(param_grid=grid))\n",
    "    Parallel(n_jobs=-1)(delayed(generate_parametric)(dataset_mas, self.densities_dir, self.fits_dir, expanded_grid.iloc[i,]['clazz'], expanded_grid.iloc[i,]['transform']) for i in range(len(expanded_grid.index)))\n",
    "\n",
    "def generate_empirical_kde() -> None:\n",
    "    grid = dict(\n",
    "        clazz = [Empirical, KDE_approx],\n",
    "        transform = list(DistTransform))\n",
    "    expanded_grid = pd.DataFrame(ParameterGrid(param_grid=grid))\n",
    "    Parallel(n_jobs=min(self.num_cpus, len(expanded_grid.index)))(delayed(generate_empirical)(self.ds, self.densities_dir, expanded_grid.iloc[i,]['clazz'], expanded_grid.iloc[i,]['transform']) for i in range(len(expanded_grid.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Up\n",
    "\n",
    "We have successfully imported our own datasets, created parametric fits and densities for the web application.\n",
    "In order for the dataset to become usable, it needs to be made available in the `datasets`-folder of Metrics As Scores and follow the directory structure of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets reside currently in: C:\\repos\\lnu_metrics-as-scores\\datasets\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.__init__ import DATASETS_DIR\n",
    "\n",
    "print(f'The datasets reside currently in: {str(DATASETS_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this notebook, we will copy over the **default** dataset (a template for new datasets that comes with Metrics As Scores) and then save our manifest, the transformed data, the parametric fits, and the generated densities in that folder.\n",
    "Then, we ask a helper to check the consistency of our work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "89aca8a44dfb413f73d482f243b8ab5fc2810ff69f597b50e1e58d8574d0fd0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
