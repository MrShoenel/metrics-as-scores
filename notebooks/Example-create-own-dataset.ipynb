{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: Create Own Dataset\n",
    "=================================\n",
    "\n",
    "In this example, we will demonstrate how to use an own dataset with Metrics As Scores.\n",
    "Note that it is not required to interact with code, as the following can be fully accomplished using the Text-based Command Line User Interface (**TUI**).\n",
    "It can be launched after installation simply by typing `mas` at the prompt.\n",
    "\n",
    "However, this example might still be useful for programmatic usage, when you need to create datasets in a batch fashion.\n",
    "\n",
    "Please note: The documentation for the code can be found at <https://mrshoenel.github.io/metrics-as-scores/>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Creation of Own Dataset in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will go through the following steps:\n",
    "\n",
    "1. Load the Iris Data Frame and Transform It Into the Required Format\n",
    "   1. Transforming the Data Frame\n",
    "   2. Creating a Manifest\n",
    "   3. Using the Workflow to Initialize the Dataset\n",
    "2. Conduct Analyses That Are Required for Generating a Scientific Report\n",
    "3. Fitting of Parametric Random Variables to the Data\n",
    "4. Generating the Densities Required for the Web Application of Metrics As Scores\n",
    "5. Finishing Up: Prepare for Publication\n",
    "   1. Rendering the About.pdf With Quarto\n",
    "   2. Bundling the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Iris Data Frame and Transform It Into the Required Format\n",
    "\n",
    "The well-known Iris dataset is included in `scikit-learn`, so we can load it directly.\n",
    "It has 4 real-valued features and one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "\n",
    "# Let's rename the numeric column and use the actual labels:\n",
    "for idx in range(len(iris.target_names)):\n",
    "    df['target'].replace(to_replace=idx, value=iris.target_names[idx], inplace=True)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Transforming the Data Frame\n",
    "\n",
    "Metrics As Scores requires a stacked format of the data frame. We take each feature's data, the group, and the name of the feature to produce a new 3-column data frame. After we have done this for all features, we vertically stack these frames. This is implemented as a helper function, so we can just go ahead and make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature   Group  Value\n",
      "0  sepal length (cm)  setosa    5.1\n",
      "1  sepal length (cm)  setosa    4.9\n",
      "2  sepal length (cm)  setosa    4.7\n",
      "3  sepal length (cm)  setosa    4.6\n",
      "4  sepal length (cm)  setosa    5.0\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.tools.funcs import transform_to_MAS_dataset\n",
    "\n",
    "df_mas = transform_to_MAS_dataset(df=df, group_col='target', feature_cols=iris.feature_names)\n",
    "\n",
    "print(df_mas.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Creating a Manifest\n",
    "\n",
    "Every dataset must be accompanied by a manifest that provides some meta information about it. The manifest is also required for the next steps. When bundling and publishing a dataset, the manifest is a required file.\n",
    "The dataset's ID in the manifest will also be used as **dataset's local folder's name**.\n",
    "\n",
    "Metrics As Scores used a typed dictionary that is then stored as `manifest.json`. Then, a new instance of `Dataset` is created. It is passed the transformed data frame and the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': ['First A. Author', 'Second Author'],\n",
       " 'name': 'The Iris dataset.',\n",
       " 'id': 'iris-example',\n",
       " 'desc': 'The Iris dataset holds 50 observations per species.',\n",
       " 'colname_context': 'Group',\n",
       " 'colname_data': 'Value',\n",
       " 'colname_type': 'Feature',\n",
       " 'desc_qtypes': {'sepal length (cm)': 'The sepal length (cm)',\n",
       "  'sepal width (cm)': 'The sepal width (cm)',\n",
       "  'petal length (cm)': 'The petal length (cm)',\n",
       "  'petal width (cm)': 'The petal width (cm)'},\n",
       " 'qtypes': {'sepal length (cm)': 'continuous',\n",
       "  'sepal width (cm)': 'continuous',\n",
       "  'petal length (cm)': 'continuous',\n",
       "  'petal width (cm)': 'continuous'},\n",
       " 'contexts': ['setosa', 'versicolor', 'virginica'],\n",
       " 'ideal_values': {'sepal length (cm)': None,\n",
       "  'sepal width (cm)': None,\n",
       "  'petal length (cm)': None,\n",
       "  'petal width (cm)': None}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_as_scores.distribution.distribution import Dataset, LocalDataset\n",
    "\n",
    "# Here, we manually create a manifest and fill out the minimum required properties.\n",
    "# The dataset creation workflow has a nice wizard that we can use, otherwise.\n",
    "# A manifest for a publishable dataset needs to present proper values for all keys.\n",
    "manifest: LocalDataset = {}\n",
    "manifest['author'] = ['First A. Author', 'Second Author']\n",
    "manifest['name'] = 'The Iris dataset'\n",
    "manifest['id'] = 'iris-example'\n",
    "manifest['desc'] = 'The Iris dataset holds 50 observations per species.'\n",
    "manifest['colname_context'] = 'Group'\n",
    "manifest['colname_data'] = 'Value'\n",
    "manifest['colname_type'] = 'Feature'\n",
    "# Note this is a dictionary, where the keys are the features' (column) names\n",
    "# and the values are a obligatory descriptions.\n",
    "manifest['desc_qtypes'] = { fn: f'The {fn}' for fn in iris.feature_names }\n",
    "# The feature (quantity) types is a dictionary, too:\n",
    "manifest['qtypes'] = { fn: 'continuous' for fn in iris.feature_names }\n",
    "# This is a list of the available groups.\n",
    "manifest['contexts'] = iris.target_names.tolist()\n",
    "# The Iris dataset has no ideal values for any of its features:\n",
    "manifest['ideal_values'] = { fn: None for fn in iris.feature_names }\n",
    "\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Using the Workflow to Initialize the Dataset\n",
    "\n",
    "In order to create a `Dataset`, we need the manifest and the original data frame.\n",
    "We will use the workflow that is also used in the TUI to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset's folder will be: C:\\repos\\lnu_metrics-as-scores\\datasets\\iris-example\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.cli.CreateDataset import CreateDatasetWorkflow\n",
    "\n",
    "create = CreateDatasetWorkflow(manifest=manifest, org_df=df_mas)\n",
    "# Let's also get the dataset as initialized by the workflow.\n",
    "dataset = create.dataset\n",
    "print(f'The dataset\\'s folder will be: {str(create.dataset_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the workflow, we will exploit its convenience methods to initialize the dataset.\n",
    "First, we will make sure all required directories exist, before we copy copy over some files from the *default* dataset and save the manifest and data frame to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create._make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create._init_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create._save_manifest_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and check out the directory that was created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Conduct Analyses That Are Required for Generating a Scientific Report\n",
    "\n",
    "Here, we will conduct the three statistical analyses that are required to generate a report.\n",
    "The report itself is a Quarto template that can (but does not necessarily have to) be changed.\n",
    "\n",
    "You can call the methods on the `Dataset` manually or use the workflow to run the analyses.\n",
    "Here, for demonstration purposes, we call them one-by-one and save the files manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 160.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtype</th>\n",
       "      <th>stat</th>\n",
       "      <th>pval</th>\n",
       "      <th>across_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>44.194514</td>\n",
       "      <td>1.250402e-23</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>24.727043</td>\n",
       "      <td>2.637466e-14</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>87.738074</td>\n",
       "      <td>1.219770e-40</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>85.564667</td>\n",
       "      <td>6.874646e-40</td>\n",
       "      <td>setosa;versicolor;virginica;__ALL__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qtype       stat          pval  \\\n",
       "0  sepal length (cm)  44.194514  1.250402e-23   \n",
       "1   sepal width (cm)  24.727043  2.637466e-14   \n",
       "2  petal length (cm)  87.738074  1.219770e-40   \n",
       "3   petal width (cm)  85.564667  6.874646e-40   \n",
       "\n",
       "                       across_contexts  \n",
       "0  setosa;versicolor;virginica;__ALL__  \n",
       "1  setosa;versicolor;virginica;__ALL__  \n",
       "2  setosa;versicolor;virginica;__ALL__  \n",
       "3  setosa;versicolor;virginica;__ALL__  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we want to do a full analysis and, therefore, pass in all available features\n",
    "# (called quantity types) and groups (called contexts). We also pass in a virtual \"ALL\"-\n",
    "# context, which creates an additional group that contains all data.\n",
    "test_anova = dataset.analyze_ANOVA(\n",
    "    qtypes=dataset.quantity_types, contexts=list(dataset.contexts(include_all_contexts=True)))\n",
    "\n",
    "# Let's save the result (normally done by the workflow):\n",
    "test_anova.to_csv(create.path_test_ANOVA, index=False)\n",
    "\n",
    "test_anova.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1997.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>meandiff</th>\n",
       "      <th>p-adj</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>reject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>setosa</td>\n",
       "      <td>-0.8373</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.1287</td>\n",
       "      <td>-0.5460</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.8441</td>\n",
       "      <td>-0.1987</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__ALL__</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.7447</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4533</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>setosa</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>1.2868</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>setosa</td>\n",
       "      <td>virginica</td>\n",
       "      <td>1.5820</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2252</td>\n",
       "      <td>1.9388</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group1      group2  meandiff   p-adj   lower   upper  reject\n",
       "0  __ALL__      setosa   -0.8373  0.0000 -1.1287 -0.5460    True\n",
       "1  __ALL__  versicolor    0.0927  0.8441 -0.1987  0.3840   False\n",
       "2  __ALL__   virginica    0.7447  0.0000  0.4533  1.0360    True\n",
       "3   setosa  versicolor    0.9300  0.0000  0.5732  1.2868    True\n",
       "4   setosa   virginica    1.5820  0.0000  1.2252  1.9388    True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tukey's Honest Significance Test:\n",
    "test_tukey = dataset.analyze_TukeyHSD(qtypes=dataset.quantity_types)\n",
    "test_tukey.to_csv(create.path_test_TukeyHSD, index=False)\n",
    "test_tukey.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4036.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtype</th>\n",
       "      <th>stat</th>\n",
       "      <th>pval</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.807571e-15</td>\n",
       "      <td>setosa</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.773164e-23</td>\n",
       "      <td>setosa</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.537645e-11</td>\n",
       "      <td>setosa</td>\n",
       "      <td>__ALL__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.807534e-06</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.411500e-02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>__ALL__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               qtype  stat          pval      group1      group2\n",
       "0  sepal length (cm)  0.78  2.807571e-15      setosa  versicolor\n",
       "1  sepal length (cm)  0.92  7.773164e-23      setosa   virginica\n",
       "2  sepal length (cm)  0.56  2.537645e-11      setosa     __ALL__\n",
       "3  sepal length (cm)  0.50  4.807534e-06  versicolor   virginica\n",
       "4  sepal length (cm)  0.24  2.411500e-02  versicolor     __ALL__"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The two-sample Komolgorov--Smirnov Test:\n",
    "test_ks2 = dataset.analyze_distr(qtypes=dataset.quantity_types, use_ks_2samp=True)\n",
    "test_ks2.to_csv(create.path_test_ks2samp, index=False)\n",
    "test_ks2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fitting of Parametric Random Variables to the Data\n",
    "\n",
    "Fitting random variables is required so that we can inspect and use the parametric fits in the web application.\n",
    "A dataset that ought to be published needs to contain these.\n",
    "Metrics As Scores can fit more than ~$120$ random variables, of which ~$20$ are discrete.\n",
    "The Iris dataset only has real-valued (continuous) features, so we will limit ourselves to continuous random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_as_scores.cli.FitParametric import FitParametricWorkflow\n",
    "from metrics_as_scores.distribution.fitting import FitterPymoo\n",
    "\n",
    "fit = FitParametricWorkflow()\n",
    "# Let's manually initialize the workflow:\n",
    "fit.use_ds = manifest\n",
    "fit.ds = dataset\n",
    "fit.df = df_mas\n",
    "fit.fits_dir = create.fits_dir\n",
    "fit.selected_rvs_d = [] # Do not attempt fitting any discrete random variables\n",
    "fit.num_cpus = -1 # Use all available CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_as_scores.distribution.fitting import Continuous_RVs # There is also 'Discrete_RVs' if you need them.\n",
    "\n",
    "# Note that this is a list of random variables that were previously observed to best fit\n",
    "# (transformations of) the Iris data, it is not just made up!\n",
    "from scipy.stats._continuous_distns import alpha_gen, cauchy_gen, crystalball_gen, exponnorm_gen, fisk_gen, foldcauchy_gen, genlogistic_gen, gumbel_l_gen, gumbel_r_gen, johnsonsb_gen, johnsonsu_gen, kstwobign_gen, laplace_asymmetric_gen, moyal_gen, nakagami_gen, ncf_gen, nct_gen, norm_gen, pearson3_gen, rayleigh_gen, rdist_gen, reciprocal_gen, rice_gen, skew_norm_gen, truncweibull_min_gen\n",
    "\n",
    "# Let's use this short list in the fitting workflow:\n",
    "fit.selected_rvs_c = [alpha_gen, cauchy_gen, crystalball_gen, exponnorm_gen, fisk_gen, foldcauchy_gen, genlogistic_gen, gumbel_l_gen, gumbel_r_gen, johnsonsb_gen, johnsonsu_gen, kstwobign_gen, laplace_asymmetric_gen, moyal_gen, nakagami_gen, ncf_gen, nct_gen, norm_gen, pearson3_gen, rayleigh_gen, rdist_gen, reciprocal_gen, rice_gen, skew_norm_gen, truncweibull_min_gen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the fitting process!\n",
    "\n",
    "For each `DistTransform`, we generate a set of parametric fits.\n",
    "Instead of attempting to fit all available continuous random variables (from `Continuous_RVs`), we use a dedicated list of random variables that should be attempted fitting, just for the sake of computing this notebook faster.\n",
    "In reality you should always attempt to fit them all, and you even must do so if you intend to publish your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform <none>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4006.98it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2005.17it/s]\n",
      "100%|██████████| 400/400 [00:04<00:00, 92.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform E[X] (expectation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3988.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4033.95it/s]\n",
      "100%|██████████| 400/400 [00:04<00:00, 99.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform Median (50th percentile)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1977.51it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4005.06it/s]\n",
      "100%|██████████| 400/400 [00:04<00:00, 98.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform Mode (most likely value)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3993.62it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4010.81it/s]\n",
      "100%|██████████| 400/400 [00:04<00:00, 89.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform Infimum (min. observed value)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2002.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2000.86it/s]\n",
      "100%|██████████| 400/400 [00:03<00:00, 104.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting of distributions for transform Supremum (max. observed value)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2002.77it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4006.02it/s]\n",
      "100%|██████████| 400/400 [00:03<00:00, 109.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.distribution.distribution import DistTransform\n",
    "from pickle import dump\n",
    "from typing import Any\n",
    "\n",
    "result: list[dict[str, Any]] = None\n",
    "for dist_transform in list(DistTransform):\n",
    "    # The following will compute all fits for all features for a single DistTransform\n",
    "    # and save the file in the dataset's 'fits'-folder.\n",
    "    print(f'Starting fitting of distributions for transform {dist_transform.value}')\n",
    "    result = fit._fit_parametric(dist_transform=dist_transform, do_print=False)\n",
    "\n",
    "    result_file = fit.fits_dir.joinpath(f'./pregen_distns_{dist_transform.name}.pickle')\n",
    "    with open(file=str(result_file), mode='wb') as fp:\n",
    "        dump(obj=result, file=fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a fit.\n",
    "You see that some best-fitting random variable was found (selected by the most appropriate statistical test).\n",
    "A variety of statistical tests was performed and each `FitResult` retains all of their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'setosa',\n",
       " 'dist_transform': 'SUPREMUM',\n",
       " 'qtype': 'sepal width (cm)',\n",
       " 'rv': 'fisk_gen',\n",
       " 'type': 'continuous',\n",
       " 'grid_idx': 29,\n",
       " 'transform_value': 4.4,\n",
       " 'params': {'c': 60517527.07863054,\n",
       "  'loc': -12641545.884248791,\n",
       "  'scale': 12641546.864645472},\n",
       " 'stat_tests': {'tests': {'cramervonmises_ordinary': {'pval': 0.7619894052317475,\n",
       "    'stat': 0.06867291332303076},\n",
       "   'cramervonmises_jittered': {'pval': 0.7619894052317475,\n",
       "    'stat': 0.06867291332303076},\n",
       "   'cramervonmises_2samp_ordinary': {'pval': 0.8343025940612143,\n",
       "    'stat': 0.05900000000000105},\n",
       "   'cramervonmises_2samp_jittered': {'pval': 0.8343025940612143,\n",
       "    'stat': 0.05900000000000105},\n",
       "   'ks_1samp_ordinary': {'pval': 0.7033899762790617,\n",
       "    'stat': 0.09655598332815263},\n",
       "   'ks_1samp_jittered': {'pval': 0.7033899762790617,\n",
       "    'stat': 0.09655598332815263},\n",
       "   'ks_2samp_ordinary': {'pval': 0.9667464356809096, 'stat': 0.1},\n",
       "   'ks_2samp_jittered': {'pval': 0.9667464356809096, 'stat': 0.1},\n",
       "   'epps_singleton_2samp_ordinary': {'pval': 0.8082240993192712,\n",
       "    'stat': 1.6031598468584338},\n",
       "   'epps_singleton_2samp_jittered': {'pval': 0.8082240993192712,\n",
       "    'stat': 1.6031598468584338}},\n",
       "  'discrete_data1': False,\n",
       "  'discrete_data2': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, go ahead and check out the generated files in the `fits`-folder of your new dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generating the Densities Required for the Web Application of Metrics As Scores\n",
    "\n",
    "In order to use web application with our dataset, we need to pre-generate densities for it, as generating these during runtime could take a lot of time (depending on the dataset size) and be detrimental to the user experience.\n",
    "Therefore, we pre-generate these files and trade storage space for computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_as_scores.cli.GenerateDensities import GenerateDensitiesWorkflow\n",
    "\n",
    "gendens = GenerateDensitiesWorkflow()\n",
    "gendens.use_ds = manifest\n",
    "gendens.ds = dataset\n",
    "# We need access to them for generating densities from parametric fits\n",
    "gendens.fits_dir = create.fits_dir\n",
    "gendens.densities_dir = create.densities_dir\n",
    "gendens.num_cpus = -1 # Allow parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to pre-generate all densities.\n",
    "Note that you should always generate all of the following, even if you do not have discrete features, because the process will generate empty densities and mark the distributions explicitly as *unfit*.\n",
    "This is then exploited in the web application.\n",
    "\n",
    "The following generates densities for five different types and six different distribution transforms (a total of $30$ files will be created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendens._generate_parametric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendens._generate_empirical_kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendens._generate_empirical_discrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the `densities`-folder in your dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Finishing Up: Prepare for Publication\n",
    "\n",
    "We have successfully imported our own datasets, created parametric fits and densities for the web application.\n",
    "We should now run a consistency check.\n",
    "The result of it should be that only one last file is missing: The **``About.pdf``**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_as_scores.cli.helpers import required_files_folders_local_dataset, validate_local_dataset_files, PathStatus\n",
    "\n",
    "dirs, files = required_files_folders_local_dataset(local_ds_id=manifest['id'])\n",
    "dirs_status, files_status = validate_local_dataset_files(dirs=dirs, files=files)\n",
    "\n",
    "for dir in dirs_status:\n",
    "    assert dirs_status[dir] == PathStatus.OK\n",
    "\n",
    "for file in files_status:\n",
    "    if 'About.pdf' in file.name:\n",
    "        assert files_status[file] == PathStatus.DOESNT_EXIST\n",
    "    else:\n",
    "        assert files_status[file] == PathStatus.OK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Rendering the About.pdf With Quarto\n",
    "\n",
    "You will find the files `About.qmd` and `_quarto.yml` in your newly created dataset folder.\n",
    "Those will use the dataset's manifest and statistical tests generated earlier to render a nice-looking scientific report that shall accompany your dataset.\n",
    "It is supposed to be published alongside the dataset.\n",
    "\n",
    "An `About.pdf` is required for publication.\n",
    "How you produce it is ultimately up to you, of course.\n",
    "In the following, we simply render the Quarto template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.18363.592]\n",
      "(c) 2019 Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(venv) c:\\repos\\lnu_metrics-as-scores\\notebooks>quarto render ../datasets/iris-example/About.qmd --to pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting python3 kernel...Done\n",
      "\n",
      "Executing 'About.ipynb'\n",
      "  Cell 1/13...Done\n",
      "  Cell 2/13...Done\n",
      "  Cell 3/13...Done\n",
      "  Cell 4/13...Done\n",
      "  Cell 5/13...Done\n",
      "  Cell 6/13...Done\n",
      "  Cell 7/13...Done\n",
      "  Cell 8/13...Done\n",
      "  Cell 9/13...Done\n",
      "  Cell 10/13...Done\n",
      "  Cell 11/13...Done\n",
      "  Cell 12/13...Done\n",
      "  Cell 13/13...Done\n",
      "\n",
      "pandoc \n",
      "  to: latex\n",
      "  output-file: About.tex\n",
      "  standalone: true\n",
      "  pdf-engine: pdflatex\n",
      "  variables:\n",
      "    graphics: true\n",
      "    tables: true\n",
      "  default-image-extension: pdf\n",
      "  number-sections: true\n",
      "  top-level-division: section\n",
      "  \n",
      "metadata\n",
      "  documentclass: scrartcl\n",
      "  classoption:\n",
      "    - DIV=11\n",
      "    - numbers=noendperiod\n",
      "  papersize: letter\n",
      "  header-includes:\n",
      "    - '\\KOMAoption{captions}{tableheading}'\n",
      "  block-headings: true\n",
      "  bibliography:\n",
      "    - refs.bib\n",
      "  title: The Iris dataset.\n",
      "  author:\n",
      "    - First A. Author\n",
      "    - Second Author\n",
      "  geometry: 'a4paper,margin=2.25cm'\n",
      "  subtitle: A Dataset For _Metrics As Scores_\n",
      "  jupyter: python3\n",
      "  \n",
      "running pdflatex - 1\n",
      "  This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) (preloaded format=pdflatex)\n",
      "   restricted \\write18 enabled.\n",
      "  entering extended mode\n",
      "  \n",
      "running pdflatex - 2\n",
      "  This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) (preloaded format=pdflatex)\n",
      "   restricted \\write18 enabled.\n",
      "  entering extended mode\n",
      "  \n",
      "\n",
      "Output created: About.pdf\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(venv) c:\\repos\\lnu_metrics-as-scores\\notebooks>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "quarto render ../datasets/iris-example/About.qmd --to pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Bundling the Dataset\n",
    "\n",
    "Now that the dataset is complete, we can bundle it into a single Zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file created at: C:\\repos\\lnu_metrics-as-scores\\datasets\\iris-example\\dataset.zip\n"
     ]
    }
   ],
   "source": [
    "from metrics_as_scores.cli.BundleOwn import BundleDatasetWorkflow\n",
    "\n",
    "bundle = BundleDatasetWorkflow()\n",
    "bundle.use_ds = manifest\n",
    "bundle.ds_dir = create.dataset_dir\n",
    "\n",
    "zip = bundle._make_zip()\n",
    "print(f'Zip file created at: {str(zip)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When uploading the dataset, it is recommended to upload the `dataset.zip` file alongside the `About.pdf`, so it is possible to get an overview of the dataset before downloading it.\n",
    "Here are three example datasets:\n",
    "\n",
    "- Metrics and Domains From the Qualitas.class corpus (Hönel 2023b). 10 GB. <https://doi.org/10.5281/zenodo.7633949>.\n",
    "- ELISA Spectrophotometer Samples (Hönel 2023a). 266 MB. <https://doi.org/10.5281/zenodo.7633989>.\n",
    "- Price, weight, and other properties of over 1,200 ideal-cut and best-clarity diamonds (Hönel 2023c). 508 MB. <https://doi.org/10.5281/zenodo.7647596>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "89aca8a44dfb413f73d482f243b8ab5fc2810ff69f597b50e1e58d8574d0fd0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
