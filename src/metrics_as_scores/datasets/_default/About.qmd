---
subtitle: "A Dataset For _Metrics As Scores_"
jupyter: python3
---

\newcommand{\abs}[1]{\left\lvert\,#1\,\right\rvert}
\newcommand\tightto{\!\to\!}
\newcommand\tightmapsto{\!\mapsto\!}
\newcommand{\tight}[1]{\,{#1}\,}
\newcommand{\utight}[1]{{#1}\,}

```{python}
#| echo: false

# You may edit this; it's the significance level used in this report.
use_alpha = 0.05

import pandas as pd
from os import getcwd
from pathlib import Path
from json import load
from IPython.display import display, Markdown
from tabulate import tabulate
from metrics_as_scores.distribution.distribution import Dataset, LocalDataset

def format_comma_and(lst: str) -> str:
    if len(lst) == 1:
        return lst[0]
    l = lst[0:(len(lst) - 1)]
    r = lst[len(lst) - 1]
    if len(l) == 1:
        return f'{l[0]} and {r}'
    return f'{", ".join(l)}, and {r}'

dm = lambda s: display(Markdown(s))

# Let's load the entire dataset, except for the original df!
dataset_dir = Path(getcwd())

df_anova = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/anova.csv')))
df_ks2samp = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/ks2samp.csv')))
df_tukey = pd.read_csv(filepath_or_buffer=str(dataset_dir.joinpath('./stat-tests/tukeyhsd.csv')))
manifest: LocalDataset = None
with open(file=str(dataset_dir.joinpath('./manifest.json')), mode='r', encoding='utf-8') as fp:
	manifest = load(fp=fp)

dataset = Dataset(ds=manifest, df=pd.DataFrame())
```


# Description

<!-- You can put more description here, into the following diplayed markup, or below. -->

```{python}
#| echo: false

dm(f"""
{manifest['desc']}
""")
```


This dataset has the following **Features**:

```{python}
#| echo: false

from metrics_as_scores.tools.funcs import natsort

qtypes = dataset.quantity_types
qtypes.sort(key=natsort)

temp = [f'- *{qtype}*: {dataset.qytpe_desc(qtype=qtype)}{" [discrete]" if dataset.is_qtype_discrete(qtype=qtype) else ""}' for qtype in qtypes]

dm('\n'.join(temp))
```

```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))
contexts.sort(key=natsort)

dm(f'''
It has a total of {len(contexts)} **Groups**: {format_comma_and(list([f'*{c}*' for c in contexts]))}.''')
```



# Analysis

In this section, results for the analysis of variance (ANOVA) and Tukey's Honest Significance Test (TukeyHSD) are shown.
These tests will give a first indication as to how different the features are across groups.
These two tests were used in the original paper [@honel2022qrs] that *Metrics As Scores* was initially conceived for.


These tests are conducted to help answering related questions, such as:

- Are there significant statistical differences for each feature across all groups?
- Is each group in its entirety (i.e., considering all features) distinguishable from the other groups?
- What are good/bad or common/extreme scores for each group of the given dataset?


## ANOVA

This test analyzes the differences among means [@Chambers2017-fu].
For each feature, this test analyzes if means of its samples are significantly different across groups.
The null hypothesis of this test is that there are *no* significant differences.
This test yields a p-value and an F-statistic.
The latter is the mean square of each independent variable divided by the mean square of the residuals.
Large F-statistics indicate that the variation among groups is likely.
The p-value then indicates how likely it is for the F-statistic to have occurred, given the null hypothesis is true.


## KS2

The Two-sample Kolmogorov--Smirnov Test (KS2) is a non-parametric and tests whether two samples stem from the same probability distribution [@stephens1974kstest].
KS2 does not check for a certain type of probability distribution since it uses the samples' empirical CDFs.
Its test statistic is the maximum vertical distance between the two CDFs.
For two samples $\mathbf{x},\mathbf{y}$, the statistic is calculated as $D_{\mathbf{x},\mathbf{y}}\tight{=}\sup_t\,\abs{F_{\mathcal{X}}(t)\tight{-}F_{\mathcal{Y}}(t)}$.
The null hypothesis is that the samples' CDFs are identical, that is, $F_{\mathcal{X}}\tight{=}F_{\mathcal{Y}}$.
This test is used to compare one feature between two groups.


## TukeyHSD

This test is used to gain insights into the results of an ANOVA test.
While the former only allows obtaining the amount of corroboration for the null hypothesis, TukeyHSD performs all pairwise comparisons [@tukey1949hsd].
For example, by choosing a certain feature and group, we obtain a list of other groups that are significantly statistically different.
The null hypothesis of this test is the same as for the ANOVA test.


# Results

Here we present some insights from conducting the ANOVA- KS2-, and TukeyHSD tests.


## ANOVA

@tbl-anova show the results of the ANOVA analysis.
For each feature, it indicates whether the features' means vary significantly across groups.
For this test, we also add a virtual group, in which we simply merge the values of all groups and effectively disregard the group attribute.
Therefore, the ANOVA test also indicates whether the features' values are different in a specific group when compared to all recorded values.


```{python}
#| echo: false

contexts = list(dataset.contexts(include_all_contexts=False))
contexts.sort(key=natsort)
row = df_anova.iloc[0,:].to_dict()


null_holds = row["pval"] > use_alpha
interpret: str=None
if null_holds:
	interpret = f'For a significance level of $\\alpha\\tight{{=}}{use_alpha}$, the null hypothesis cannot be rejected, meaning that the means of samples of the feature {row["qtype"]} are not statistically significantly different across groups.'
else:
	interpret = f'We cannot accept the null hypothesis for a significance level of $\\alpha\\tight{{=}}{use_alpha}$, meaning that the the means of samples of the feature {row["qtype"]} have statistically significantly different means across groups.'


dm(f'''
@tbl-anova shows, for each feature, if there was corroboration for the null hypothesis, which here is whether the samples of the same feature have the same mean across groups.
So, for example, samples of the first feature **{row["qtype"]}** were compared across
groups {format_comma_and(list([f"*{c}*" for c in row["across_contexts"].split(";")]))}.
{interpret}
'''.strip())
```



```{python}
#| label: tbl-anova
#| tbl-cap: Results of the ANOVA analysis.
#| echo: false

temp = df_anova[['qtype', 'pval', 'stat']]
Markdown(tabulate(tabular_data=temp, headers=['Feature', 'p-Value', 'F-Statistic'], showindex=False))
```



## KS2

```{python}
#| echo: false

n = len(contexts) + 1
r = int(n * (n - 1) / 2)

dm(f'''
The two-sample Kolmogorov--Smirnov test checks if two samples stem from an identical distribution (which is the null hypothesis).
The KS2 test requires pairwise comparisons.
For each feature, we compare it within all the groups.
Therefore, all pair-wise combinations are computed.
This dataset has ${n-1}$ groups plus one virtual (in which we effectively disregard the group, therefore, $n\\tight{{=}}{n}$).
The number of pair-wise comparisons per feature, therefore, is $n\\tight{{\\times}}(n\\tight{{-}}1)\\tight{{\\div}}2\\tight{{=}}{r}$.
In other words, each feature can significantly stick out anywhere between zero and ${r}$ times.
@fig-ks2 shows the results of the KS2 test using the significance threshold $\\alpha\\tight{{=}}{use_alpha}$.
'''.strip())
```


```{python}
#| label: fig-ks2
#| fig-cap: The frequency with which features were considered to come from the same distribution. Numbers close to the max indicate that the feature is not significantly different across groups.
#| echo: false
#| warning: false

import matplotlib
import matplotlib.pyplot as plt
from bokeh.palettes import Category20_20
from math import ceil

temp = df_ks2samp[df_ks2samp.pval >= use_alpha]
cnts = list([
	temp[(temp.qtype == qtype)].shape[0] for qtype in qtypes
])

fig, ax = plt.subplots()
fig.set_figwidth(8)
fig.set_figheight(2.5)
bars = ax.bar(qtypes, cnts, width=.5, color=Category20_20)

ax.set_ylim(bottom=0, top=r + ceil(r*.1))
ax.bar_label(bars)
ax.set_axisbelow(True)
ax.grid(color='#dddddd', linestyle='dashed')
angle = 0 if len(contexts) < 10 else (45 if len(contexts) < 20 else 90)
ax.set_xticklabels(ax.get_xticklabels(), rotation=angle, ha='right', rotation_mode='anchor')

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Computer Modern Roman"
})
plt.title("Frequency with which features are similar across groups.")
plt.show()
```



## TukeyHSD


```{python}
#| echo: false

n = len(contexts) + 1
m = len(qtypes)
r = (n-1) * m

dm(f'''
Per group, this test allows us to report how many features are different, compared to all other groups' features.
Consider the set of groups $\\mathit{{G}}$ and the set of features $\\mathit{{F}}$.
Given a group $g_i$ and a feature $f_j$, this test reports all other groups $\\mathit{{G}}^{{\\setminus g_i}}$ that have a statistically significantly different distribution for $f_j$.
However, for $g_i$, we aggregate these counts across all features.
We have a total of $n\\tight{{=}}{n-1}+1$ groups (including the virtual group).
Therefore, the number of features ($m\\tight{{=}}{len(qtypes)}$) that are different in different groups can maximally be $(n\\tight{{-}}1)\\tight{{\\times}}m\\tight{{=}}{r}$.
'''.strip())
```

```{python}
#| label: fig-tukey
#| fig-cap: Number of features that are different per group, by comparing each group to all other groups.
#| echo: false
#| warning: false

import numpy as np

temp = list()
for idx in range(len(df_tukey.index)):
	row = df_tukey.iloc[idx,:].to_dict()
	if row['reject']:
		# Reject null hypothesis -> feature's groups are different
		temp.append(row['group1'])
		temp.append(row['group2'])

values, counts = np.unique(np.array(temp), axis=0, return_counts=True)
values, counts = list(values), list(counts)

cnt_dict = {}
for idx, val in enumerate(values):
	cnt_dict[val] = counts[idx]

cnts = list()
for ctx in dataset.contexts(include_all_contexts=True):
	if ctx in cnt_dict:
		cnts.append(cnt_dict[ctx])
	else:
		cnts.append(0)

fig, ax = plt.subplots()
fig.set_figwidth(8)
fig.set_figheight(2.5)
bars = ax.bar(list(dataset.contexts(include_all_contexts=False)) + ['\\textbf{ALL}'], cnts, width=.5, color=Category20_20)

r = max(list(cnt_dict.values()))
ax.set_ylim(bottom=0, top=r + ceil(r*.1))
ax.bar_label(bars)
ax.set_axisbelow(True)
ax.grid(color='#dddddd', linestyle='dashed')
angle = 0 if len(contexts) < 10 else (45 if len(contexts) < 20 else 90)
ax.set_xticklabels(ax.get_xticklabels(), rotation=angle, ha='right', rotation_mode='anchor')

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Computer Modern Roman"
})
plt.title("Number of features different per group.")
plt.show()
```


## Common/extreme Values

```{python}
#| echo: false

use_qty = qtypes[0]

dm(f'''
In this section, we demonstrate some common or extreme values by showing what (maximum) distance is required to achieve a score as shown in header of @tbl-common-extreme.
The example shown here is that of the first feature (**{use_qty}**), which has been transformed using each group's expection, $\\mathbb{{E}}[X]$.
Since it is practically unlikely to have uniformly distributed quantities, a linear increase in score is usually associated with a non-linear decrease in distance.
Note that the values in @tbl-common-extreme are approximate.

'''.strip())
```



```{python}
#| echo: false

# Let's read the file 'densities_KDE_approx_EXPECTATION' so that we can generate some table.
# This is one example of how to do this, but feel free to generate any kind of table, using
# any type of density, or even show multiple tables.

from metrics_as_scores.distribution.distribution import KDE_approx
from pickle import load, dump

dens: dict[str, KDE_approx] = None
with open(file=str(dataset_dir.joinpath('./densities/densities_KDE_approx_EXPECTATION.pickle')), mode='rb') as fp:
	dens = load(file=fp)
```

```{python}
#| label: tbl-common-extreme
#| tbl-cap: The required maximum distance from the group's expectation $\mathbb{E}[X]$ to achieve a score less than or equal to $x$. The __ALL__-group is shown as well.
#| echo: false

import numpy as np
from typing import Callable
from scipy.optimize import direct
from scipy.integrate import quad
from itertools import product
from joblib import Parallel, delayed

# Quantiles [8+1]: 0, 0.1, 0.25, 0.5, 0.75, 0.85, 0.95, 0.99, E[X]
use_quantiles = [0.001, 0.1, 0.2, 0.33, 0.5, 0.66, 0.75, 0.85, 0.9, 0.999]
contexts = list(dataset.contexts(include_all_contexts=True))

def find_score_pdf(score: float, dist: KDE_approx) -> float:
	pdf_scale, _ = quad(func=dist.pdf, a=0, b=dist._range_data[1] * 10)
	pdf = lambda x: dist.pdf(x) / pdf_scale
	ccdf = lambda x: 1.0 - quad(func=pdf, a=0, b=x)[0]
	res = direct(func=lambda x: (score - ccdf(x))**2, bounds=((0,dist._range_data[1] * 10),))
	return res.x[0]


def compute_cell(c_idx: int, q_idx: int) -> tuple[int, int, str]:
	dist = dens[f'{contexts[c_idx]}_{use_qty}']
	return (c_idx, q_idx, '{:.2e}'.format(find_score_pdf(score=use_quantiles[q_idx], dist=dist)))


df: pd.DataFrame = None
df_file = dataset_dir.joinpath('./df-common-extreme.pickle')
if df_file.exists():
	with open(file=str(df_file), mode='rb') as fp:
		df = load(file=fp)
else:
	res = np.zeros(shape=(len(contexts), len(use_quantiles)))
	for tpl in Parallel(n_jobs=-1)(delayed(compute_cell)(perm[0], perm[1]) for perm in product(range(len(contexts)), range(len(use_quantiles)))):
		res[tpl[0], tpl[1]] = tpl[2]

	df = pd.concat([
		pd.DataFrame(contexts),
		pd.DataFrame(res),
		pd.DataFrame(list([dens[f'{ctx}_{use_qty}'].transform_value for ctx in contexts]))
	], axis=1)
	df.columns = [dataset.ds['colname_context']] + list(str(s) for s in list(use_quantiles) + ['$\\mathbb{E}[X]$'])

	with open(file=str(df_file), mode='wb') as fp:
		dump(obj=df, file=fp)

Markdown(tabulate(tabular_data=df, showindex=False, headers=df.columns))
```


<!--
	The following is the cross-platform solution for correctly showing references.
-->
# References {-}

<div id="refs"></div>
